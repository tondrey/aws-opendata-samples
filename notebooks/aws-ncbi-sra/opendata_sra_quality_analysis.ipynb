{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Read Archive (SRA) Quality Analysis on AWS"
   ],
   "id": "header-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Quality Control for Genomic Sequencing Data"
   ],
   "id": "header-2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to:\n",
    "1. Search and discover SRA datasets using metadata queries\n",
    "2. Download SRA files and convert them to FASTQ format\n",
    "3. Perform comprehensive quality control analysis (FastQC-style)\n",
    "4. Store results in AWS S3 for further analysis\n",
    "\n",
    "The Sequence Read Archive (SRA) data is stored on AWS as part of the Registry of Open Data. Most SRA data is in the proprietary `.sra` format and requires the SRA Toolkit for conversion to FASTQ."
   ],
   "id": "intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important: Instance Size and Dataset Selection\n",
    "\n",
    "**Instance Requirements:**\n",
    "- **ml.t3.medium** (2 vCPU, 4GB RAM, ~$0.05/hour): Recommended for datasets <20MB\n",
    "- **ml.t3.large** (2 vCPU, 8GB RAM, ~$0.10/hour): For datasets 20-100MB\n",
    "\n",
    "**Disk Space Warning:**\n",
    "- SRA files expand **10-15x** when converted to FASTQ\n",
    "- Example: 20MB .sra → 200-300MB FASTQ files\n",
    "- Example: 187MB .sra → 2GB FASTQ files\n",
    "- Ensure your instance has sufficient disk space\n",
    "\n",
    "**Recommendation:**\n",
    "- Start with small datasets (<20MB) on ml.t3.medium\n",
    "- Upgrade to ml.t3.large if needed\n",
    "- This tutorial uses a 7MB dataset (works on medium)\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Stop your notebook instance when not in use\n",
    "- Delete CloudFormation stack when finished"
   ],
   "id": "instance-warning"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Installation"
   ],
   "id": "part1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "\n",
    "We'll install:\n",
    "- `pysradb`: Python package for querying SRA metadata and downloading data\n",
    "- `matplotlib`: For creating quality control visualizations\n",
    "- `biopython`: For parsing FASTQ files\n",
    "- `sra-tools`: NCBI toolkit for working with SRA data (installed via conda)"
   ],
   "id": "install-desc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Installation Output:**\n",
    "- The installation may show a warning about conda version (e.g., \"A newer version of conda exists\"). This is safe to ignore.\n",
    "- Installation output is condensed to show only the last 20 lines. Full logs are available if troubleshooting is needed.\n",
    "- Expected installation time: 3-5 minutes"
   ],
   "id": "install-note"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# Note: Installation may take 3-5 minutes and will show a conda version warning (safe to ignore)\n",
    "print('Installing packages... This may take a few minutes.')\n",
    "!pip install -q pysradb matplotlib biopython 2>&1 | grep -v 'already satisfied' || true\n",
    "!conda install -c bioconda -y sra-tools 2>&1 | tail -n 20\n",
    "print('\\n\u2713 Installation complete!')"
   ],
   "id": "install",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ],
   "id": "import-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import gzip\n",
    "import boto3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict, Counter\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress tqdm warning from pysradb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "\n",
    "from pysradb.sraweb import SRAweb\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Note: You may see a TqdmExperimentalWarning - this is safe to ignore.\")"
   ],
   "id": "imports",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure AWS S3 Storage\n",
    "\n",
    "**To find your S3 bucket name:**\n",
    "1. Go to the AWS CloudFormation console\n",
    "2. Select your stack (e.g., `opendata-sra-notebook-stack`)\n",
    "3. Click the **Outputs** tab\n",
    "4. Copy the value from the `BucketName` output\n",
    "5. Paste it in the cell below"
   ],
   "id": "s3-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Update this with your S3 bucket name from CloudFormation\n",
    "dest_bucket = 'enter_the_bucket_name_created_through_cloud_formation_here'\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "print(f\"S3 client initialized. Using bucket: {dest_bucket}\")"
   ],
   "id": "s3-config",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Search and Discover SRA Datasets\n",
    "\n",
    "We'll use pysradb to search for RNA-seq datasets. For this example, we'll use a small, commonly-used dataset."
   ],
   "id": "part2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Search for Datasets\n",
    "\n",
    "**Important**: Don't browse S3 buckets directly - they contain millions of files!\n",
    "\n",
    "Instead, use this workflow:\n",
    "1. **Search metadata** using pysradb or NCBI SRA Run Selector\n",
    "2. **Find accession numbers** that match your criteria\n",
    "3. **Download from S3** using the accession number\n",
    "\n",
    "This is much faster and more efficient than browsing S3."
   ],
   "id": "search-example-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrate metadata search workflow\n",
    "\n",
    "print(\"Method 1: Search using NCBI SRA Run Selector (Web Interface)\")\n",
    "print(\"  URL: https://www.ncbi.nlm.nih.gov/Traces/study/\")\n",
    "print(\"  - Search by organism, study, experiment type\")\n",
    "print(\"  - Filter by Size (shown in Bytes on the website), date, platform\")\n",
    "print(\"  - Download accession list\")\n",
    "print()\n",
    "\n",
    "print(\"Method 2: Search using pysradb (Python)\")\n",
    "print(\"  Example: Search for datasets by criteria\")\n",
    "print()\n",
    "\n",
    "# Initialize pysradb\n",
    "db = SRAweb()\n",
    "\n",
    "# For this tutorial, we'll use a pre-selected dataset\n",
    "print(\"For this tutorial, we selected: SRR390728\")\n",
    "print(\"\\nSelection criteria:\")\n",
    "print(\"  \u2713 Available in AWS S3\")\n",
    "print(\"  \u2713 Small size (~100MB)\")\n",
    "print(\"  \u2713 E. coli RNA-seq\")\n",
    "print(\"  \u2713 Single-end reads\")\n",
    "print(\"\\nOnce you have an accession number, download from S3:\")\n",
    "print(\"\\nS3 Path Pattern:\")\n",
    "print(\"  s3://sra-pub-run-odp/sra/{ACCESSION}/{ACCESSION}\")\n",
    "print(\"  Example: s3://sra-pub-run-odp/sra/SRR390728/SRR390728\")\n",
    "print(\"\\nNote: DRR accessions appear well-populated in S3.\")\n",
    "print(\"      SRR/ERR accessions may have varying availability.\")"
   ],
   "id": "search-example",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: DRR000019\n",
    "\n",
    "For this tutorial, we'll use **DRR000019** - a small dataset perfect for learning.\n",
    "\n",
    "**Dataset info:**\n",
    "- Size: ~7MB (expands to ~70MB FASTQ)\n",
    "- Works on: ml.t3.medium\n",
    "- Conversion time: 1-2 minutes\n",
    "\n",
    "**How to find your own datasets:**\n",
    "\n",
    "1. **Go to NCBI SRA Run Selector**: https://www.ncbi.nlm.nih.gov/Traces/study/\n",
    "2. **Search** for your organism/study (e.g., \"E. coli RNA-seq\")\n",
    "3. **Filter by size**: Click \"Add filter\" → \"Size\" → Select <50MB\n",
    "4. **Click on a run**: This shows the accession number (SRR/ERR/DRR prefix)\n",
    "5. **Copy the accession**: Example: SRR390728, DRR000019\n",
    "6. **Paste into notebook**: Replace the accession in the cell above\n",
    "\n",
    "**Recommended dataset sizes:**\n",
    "- ml.t3.medium: <20MB .sra files\n",
    "- ml.t3.large: 20-100MB .sra files\n",
    "\n",
    "**Other small datasets to try:**\n",
    "- DRR000007 (44MB)\n",
    "- DRR000019 (7MB) ← Current\n",
    "- SRR390728 (187MB) - requires ml.t3.large"
   ],
   "id": "dataset-desc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the SRA accession number\n",
    "# Using DRR000019 - a small dataset perfect for tutorials\n",
    "accession = 'DRR000019'\n",
    "\n",
    "# Initialize pysradb\n",
    "db = SRAweb()\n",
    "\n",
    "# Get metadata for this accession\n",
    "print(f\"Fetching metadata for {accession}...\\n\")\n",
    "try:\n",
    "    metadata = db.sra_metadata(accession)\n",
    "    if not metadata.empty:\n",
    "        print(f\"Dataset: {accession}\")\n",
    "        print(f\"Organism: {metadata['organism_name'].values[0]}\")\n",
    "        print(f\"Library Strategy: {metadata['library_strategy'].values[0]}\")\n",
    "        print(f\"Platform: {metadata['instrument'].values[0]}\")\n",
    "        print(f\"\\nFull metadata:\")\n",
    "        display(metadata)\n",
    "except Exception as e:\n",
    "    print(f\"Metadata query note: {e}\")\n",
    "    print(f\"Proceeding with {accession} from AWS S3...\")"
   ],
   "id": "metadata-query",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Download SRA Data from AWS S3 and Convert to FASTQ\n",
    "\n",
    "Now that we have an accession number, we'll:\n",
    "1. Download the SRA file from AWS Open Data Registry\n",
    "2. Convert to FASTQ using `fasterq-dump`\n",
    "3. Compress and upload to your S3 bucket\n",
    "\n",
    "**S3 Path Structure (discovered through testing):**\n",
    "```\n",
    "s3://sra-pub-run-odp/sra/{ACCESSION}/{ACCESSION}\n",
    "```\n",
    "\n",
    "**Example:** SRR390728 \u2192 `s3://sra-pub-run-odp/sra/SRR390728/SRR390728`\n",
    "\n",
    "**Note:** Files have NO .sra extension in S3, and NO prefix directories (SRR/SRR390/etc.)"
   ],
   "id": "part3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a directory for downloads\n",
    "os.makedirs('sra_data', exist_ok=True)\n",
    "os.chdir('sra_data')\n",
    "\n",
    "print(f\"Downloading {accession} from AWS S3...\")\n",
    "print(\"\\nUsing AWS Open Data Registry (s3://sra-pub-run-odp/)\")\n",
    "\n",
    "# CORRECT path structure (discovered through testing):\n",
    "# s3://sra-pub-run-odp/sra/{ACCESSION}/{ACCESSION}\n",
    "# Note: NO .sra extension, NO prefix directories\n",
    "s3_path = f\"s3://sra-pub-run-odp/sra/{accession}/{accession}\"\n",
    "\n",
    "print(f\"S3 Path: {s3_path}\")\n",
    "print(\"Expected time: 1-5 minutes\\n\")\n",
    "\n",
    "# Download from S3\n",
    "try:\n",
    "    subprocess.run(\n",
    "        ['aws', 's3', 'cp', s3_path, f'{accession}.sra', '--no-sign-request'],\n",
    "        check=True\n",
    "    )\n",
    "    print(f\"\\n\u2713 Downloaded {accession} from S3\")\n",
    "    \n",
    "    # Verify file\n",
    "    if os.path.exists(f\"{accession}.sra\"):\n",
    "        file_size = os.path.getsize(f\"{accession}.sra\") / (1024 * 1024)\n",
    "        print(f\"File size: {file_size:.2f} MB\")\n",
    "    else:\n",
    "        print(\"Warning: File not found after download\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nError downloading from S3: {e}\")\n",
    "    print(\"\\nThis could mean:\")\n",
    "    print(\"1. The accession doesn't exist in S3\")\n",
    "    print(\"2. Network connectivity issue\")\n",
    "    print(\"3. The accession may be in a different bucket\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(f\"  - Verify accession exists: https://www.ncbi.nlm.nih.gov/sra/{accession}\")\n",
    "    print(f\"  - Check S3: aws s3 ls s3://sra-pub-run-odp/sra/{accession}/ --no-sign-request\")\n",
    "    print(f\"  - Try different accession (DRR accessions seem well-populated)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nError: AWS CLI not found.\")\n",
    "    print(\"Install: pip install awscli\")"
   ],
   "id": "download",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert SRA to FASTQ\n",
    "# The file is a valid NCBI SRA format file (187MB)\n",
    "\n",
    "print(f\"Converting {accession}.sra to FASTQ format...\")\n",
    "print(f\"File size: {os.path.getsize(f'{accession}.sra') / (1024*1024):.1f} MB\")\n",
    "print(\"\\nThis may take 10-20 minutes for a 187MB file.\")\n",
    "print(\"Progress will be shown below...\\n\")\n",
    "\n",
    "try:\n",
    "    # Use fasterq-dump with the accession (it will find the .sra file)\n",
    "    # -e 2: use 2 threads (more stable than 4 for large files)\n",
    "    # -p: show progress\n",
    "    # -t /tmp: use temp directory (helps with space)\n",
    "    # --split-files: split paired-end reads\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['fasterq-dump', accession, '-e', '2', '-p', '-t', '/tmp', '--split-files'],\n",
    "        check=True,\n",
    "        capture_output=False,  # Show output in real-time\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\u2713 Conversion complete\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\\nfasterq-dump failed. Trying fastq-dump (slower but more reliable)...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback to fastq-dump\n",
    "        # --split-files: split paired-end\n",
    "        # --gzip: compress output\n",
    "        subprocess.run(\n",
    "            ['fastq-dump', '--split-files', accession],\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"\\n\u2713 Conversion complete using fastq-dump\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e2:\n",
    "        print(f\"\\nConversion failed: {e2}\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  - Insufficient memory (file is 187MB, needs ~1-2GB RAM)\")\n",
    "        print(\"  - Insufficient disk space\")\n",
    "        print(\"  - SageMaker instance too small (try ml.t3.large)\")\n",
    "        print(\"\\nWorkaround: Use a smaller dataset\")\n",
    "        print(\"  Try: DRR000019 (only 7MB)\")\n",
    "        raise\n",
    "\n",
    "# List generated FASTQ files\n",
    "fastq_files = sorted([f for f in os.listdir('.') if f.endswith('.fastq')])\n",
    "\n",
    "if fastq_files:\n",
    "    print(f\"\\nGenerated FASTQ files:\")\n",
    "    for f in fastq_files:\n",
    "        size = os.path.getsize(f) / (1024 * 1024)\n",
    "        print(f\"  {f} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Use first file for analysis\n",
    "    fastq_file = fastq_files[0]\n",
    "    print(f\"\\nWill use {fastq_file} for quality analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nWarning: No FASTQ files generated\")\n",
    "    print(\"Files in directory:\", os.listdir('.'))"
   ],
   "id": "convert",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compress FASTQ file to save space\n",
    "print(f\"Compressing {fastq_file}...\")\n",
    "subprocess.run(['gzip', fastq_file], check=True)\n",
    "fastq_gz = f\"{fastq_file}.gz\"\n",
    "print(f\"\u2713 Compressed to {fastq_gz}\")"
   ],
   "id": "compress",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Upload to S3\n",
    "s3_key = f\"sra_data/{accession}/{fastq_gz}\"\n",
    "print(f\"Uploading to S3: s3://{dest_bucket}/{s3_key}\")\n",
    "\n",
    "try:\n",
    "    s3.upload_file(fastq_gz, dest_bucket, s3_key)\n",
    "    print(\"\u2713 Uploaded to S3\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to S3: {e}\")"
   ],
   "id": "upload",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Quality Control Analysis\n",
    "\n",
    "We'll perform comprehensive FastQC-style quality control analysis including:\n",
    "1. Per-base quality scores\n",
    "2. Per-sequence quality distribution\n",
    "3. Per-base sequence content\n",
    "4. GC content distribution\n",
    "5. Sequence length distribution\n",
    "6. Sequence duplication levels\n",
    "7. Overrepresented sequences\n",
    "\n",
    "Each metric will include Pass/Warn/Fail indicators based on standard quality thresholds.\n",
    "\n",
    "**Scientific Basis:**\n",
    "These metrics are based on FastQC, the industry-standard tool for sequencing quality control (Andrews, 2010). FastQC is used by every major sequencing facility worldwide and cited in 10,000+ scientific publications. The quality thresholds are based on:\n",
    "- Phred quality scores (Ewing & Green, 1998)\n",
    "- Empirical validation across millions of datasets\n",
    "- Community consensus (ENCODE, NIH/NCBI guidelines)\n",
    "- Platform specifications (Illumina, PacBio, etc.)\n",
    "\n",
    "**References:**\n",
    "- Andrews, S. (2010). FastQC: A Quality Control Tool for High Throughput Sequence Data. http://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n",
    "- Ewing, B., & Green, P. (1998). Base-calling of automated sequencer traces using phred. Genome Research, 8(3), 186-194."
   ],
   "id": "part4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse FASTQ and Collect Statistics"
   ],
   "id": "parse-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parse FASTQ file and collect all statistics\n",
    "print(f\"Parsing {fastq_gz} and collecting statistics...\")\n",
    "print(\"This may take several minutes for large files.\\n\")\n",
    "\n",
    "# Data structures for QC metrics\n",
    "quality_scores = []  # List of quality score arrays\n",
    "sequence_lengths = []\n",
    "gc_contents = []\n",
    "sequences = []\n",
    "per_base_content = defaultdict(lambda: defaultdict(int))\n",
    "sequence_counts = Counter()\n",
    "\n",
    "# Parse FASTQ\n",
    "read_count = 0\n",
    "max_reads = 100000  # Limit for faster processing, remove for full analysis\n",
    "\n",
    "with gzip.open(fastq_gz, 'rt') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fastq'):\n",
    "        read_count += 1\n",
    "        if read_count > max_reads:\n",
    "            break\n",
    "        \n",
    "        # Quality scores\n",
    "        quality_scores.append(record.letter_annotations['phred_quality'])\n",
    "        \n",
    "        # Sequence length\n",
    "        seq_len = len(record.seq)\n",
    "        sequence_lengths.append(seq_len)\n",
    "        \n",
    "        # GC content\n",
    "        gc = (record.seq.count('G') + record.seq.count('C')) / seq_len * 100\n",
    "        gc_contents.append(gc)\n",
    "        \n",
    "        # Per-base content\n",
    "        for pos, base in enumerate(str(record.seq)):\n",
    "            per_base_content[pos][base] += 1\n",
    "        \n",
    "        # Sequence duplication\n",
    "        sequence_counts[str(record.seq)] += 1\n",
    "        \n",
    "        if read_count % 10000 == 0:\n",
    "            print(f\"Processed {read_count:,} reads...\", end='\\r')\n",
    "\n",
    "print(f\"\\n\u2713 Parsed {read_count:,} reads\")\n",
    "print(f\"Average read length: {np.mean(sequence_lengths):.1f} bp\")\n",
    "print(f\"Average GC content: {np.mean(gc_contents):.1f}%\")"
   ],
   "id": "parse-fastq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 1: Per-base Quality Scores"
   ],
   "id": "qc1-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate per-base quality statistics\n",
    "max_length = max(len(q) for q in quality_scores)\n",
    "per_base_quality = [[] for _ in range(max_length)]\n",
    "\n",
    "for qual_array in quality_scores:\n",
    "    for pos, q in enumerate(qual_array):\n",
    "        per_base_quality[pos].append(q)\n",
    "\n",
    "# Calculate statistics for each position\n",
    "positions = list(range(1, len(per_base_quality) + 1))\n",
    "medians = [np.median(q) for q in per_base_quality]\n",
    "q25 = [np.percentile(q, 25) for q in per_base_quality]\n",
    "q75 = [np.percentile(q, 75) for q in per_base_quality]\n",
    "q10 = [np.percentile(q, 10) for q in per_base_quality]\n",
    "q90 = [np.percentile(q, 90) for q in per_base_quality]\n",
    "\n",
    "# Determine pass/warn/fail\n",
    "median_quality = np.median(medians)\n",
    "if median_quality >= 28:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif median_quality >= 20:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.fill_between(positions, q10, q90, alpha=0.3, color='lightblue', label='10-90 percentile')\n",
    "plt.fill_between(positions, q25, q75, alpha=0.5, color='yellow', label='25-75 percentile')\n",
    "plt.plot(positions, medians, 'b-', linewidth=2, label='Median')\n",
    "plt.axhline(y=28, color='green', linestyle='--', alpha=0.5, label='Good quality (Q28)')\n",
    "plt.axhline(y=20, color='orange', linestyle='--', alpha=0.5, label='Acceptable (Q20)')\n",
    "plt.xlabel('Position in read (bp)')\n",
    "plt.ylabel('Quality score')\n",
    "plt.title(f'Per-base Quality Scores - {status}', color=color, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Median quality score: {median_quality:.1f}\")"
   ],
   "id": "qc1-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 2: Per-sequence Quality Distribution"
   ],
   "id": "qc2-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate mean quality per sequence\n",
    "mean_qualities = [np.mean(q) for q in quality_scores]\n",
    "\n",
    "# Determine status\n",
    "peak_quality = np.percentile(mean_qualities, 50)\n",
    "if peak_quality >= 30:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif peak_quality >= 20:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(mean_qualities, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=30, color='green', linestyle='--', label='Excellent (Q30)')\n",
    "plt.axvline(x=20, color='orange', linestyle='--', label='Acceptable (Q20)')\n",
    "plt.xlabel('Mean Sequence Quality (Phred Score)')\n",
    "plt.ylabel('Number of Sequences')\n",
    "plt.title(f'Per-sequence Quality Score Distribution - {status}', color=color, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Peak quality: {peak_quality:.1f}\")\n",
    "print(f\"Mean quality: {np.mean(mean_qualities):.1f}\")"
   ],
   "id": "qc2-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 3: Per-base Sequence Content"
   ],
   "id": "qc3-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate percentage of each base at each position\n",
    "positions = sorted(per_base_content.keys())\n",
    "a_pct = []\n",
    "t_pct = []\n",
    "g_pct = []\n",
    "c_pct = []\n",
    "\n",
    "for pos in positions:\n",
    "    total = sum(per_base_content[pos].values())\n",
    "    a_pct.append(per_base_content[pos]['A'] / total * 100)\n",
    "    t_pct.append(per_base_content[pos]['T'] / total * 100)\n",
    "    g_pct.append(per_base_content[pos]['G'] / total * 100)\n",
    "    c_pct.append(per_base_content[pos]['C'] / total * 100)\n",
    "\n",
    "# Check for bias (should be relatively flat)\n",
    "max_variation = max(np.std(a_pct), np.std(t_pct), np.std(g_pct), np.std(c_pct))\n",
    "if max_variation < 10:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif max_variation < 20:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "pos_range = list(range(1, len(positions) + 1))\n",
    "plt.plot(pos_range, a_pct, 'g-', label='%A', linewidth=2)\n",
    "plt.plot(pos_range, t_pct, 'r-', label='%T', linewidth=2)\n",
    "plt.plot(pos_range, g_pct, 'b-', label='%G', linewidth=2)\n",
    "plt.plot(pos_range, c_pct, 'orange', label='%C', linewidth=2)\n",
    "plt.xlabel('Position in read (bp)')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.title(f'Per-base Sequence Content - {status}', color=color, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Max variation: {max_variation:.1f}%\")"
   ],
   "id": "qc3-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 4: GC Content Distribution"
   ],
   "id": "qc4-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot GC content distribution\n",
    "mean_gc = np.mean(gc_contents)\n",
    "std_gc = np.std(gc_contents)\n",
    "\n",
    "# Theoretical normal distribution\n",
    "x = np.linspace(0, 100, 100)\n",
    "theoretical = np.exp(-0.5 * ((x - mean_gc) / std_gc) ** 2)\n",
    "theoretical = theoretical / theoretical.max() * max(np.histogram(gc_contents, bins=50)[0])\n",
    "\n",
    "# Status based on distribution shape\n",
    "if std_gc < 10:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif std_gc < 15:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(gc_contents, bins=50, edgecolor='black', alpha=0.7, label='Observed')\n",
    "plt.plot(x, theoretical, 'r--', linewidth=2, label='Theoretical')\n",
    "plt.axvline(x=mean_gc, color='blue', linestyle='--', label=f'Mean: {mean_gc:.1f}%')\n",
    "plt.xlabel('GC Content (%)')\n",
    "plt.ylabel('Number of Sequences')\n",
    "plt.title(f'GC Content Distribution - {status}', color=color, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Mean GC: {mean_gc:.1f}%\")\n",
    "print(f\"Std Dev: {std_gc:.1f}%\")"
   ],
   "id": "qc4-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 5: Sequence Length Distribution"
   ],
   "id": "qc5-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot sequence length distribution\n",
    "length_counts = Counter(sequence_lengths)\n",
    "unique_lengths = len(length_counts)\n",
    "\n",
    "# Status: should be uniform for Illumina\n",
    "if unique_lengths == 1:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif unique_lengths <= 3:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "lengths = sorted(length_counts.keys())\n",
    "counts = [length_counts[l] for l in lengths]\n",
    "plt.bar(lengths, counts, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Sequence Length (bp)')\n",
    "plt.ylabel('Number of Sequences')\n",
    "plt.title(f'Sequence Length Distribution - {status}', color=color, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Unique lengths: {unique_lengths}\")\n",
    "print(f\"Length range: {min(sequence_lengths)}-{max(sequence_lengths)} bp\")"
   ],
   "id": "qc5-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 6: Sequence Duplication Levels"
   ],
   "id": "qc6-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate duplication levels\n",
    "total_sequences = len(sequence_counts)\n",
    "unique_sequences = sum(1 for count in sequence_counts.values() if count == 1)\n",
    "duplication_rate = (1 - unique_sequences / total_sequences) * 100\n",
    "\n",
    "# Group by duplication level\n",
    "dup_levels = Counter()\n",
    "for seq, count in sequence_counts.items():\n",
    "    if count == 1:\n",
    "        dup_levels['1'] += 1\n",
    "    elif count == 2:\n",
    "        dup_levels['2'] += 1\n",
    "    elif count <= 4:\n",
    "        dup_levels['3-4'] += 1\n",
    "    elif count <= 9:\n",
    "        dup_levels['5-9'] += 1\n",
    "    elif count <= 49:\n",
    "        dup_levels['10-49'] += 1\n",
    "    else:\n",
    "        dup_levels['50+'] += 1\n",
    "\n",
    "# Status\n",
    "if duplication_rate < 20:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif duplication_rate < 50:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "levels = ['1', '2', '3-4', '5-9', '10-49', '50+']\n",
    "counts = [dup_levels[l] for l in levels]\n",
    "plt.bar(levels, counts, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Duplication Level')\n",
    "plt.ylabel('Number of Sequences')\n",
    "plt.title(f'Sequence Duplication Levels - {status}', color=color, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Total sequences: {total_sequences:,}\")\n",
    "print(f\"Unique sequences: {unique_sequences:,}\")\n",
    "print(f\"Duplication rate: {duplication_rate:.1f}%\")"
   ],
   "id": "qc6-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC Metric 7: Overrepresented Sequences"
   ],
   "id": "qc7-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Find most common sequences\n",
    "most_common = sequence_counts.most_common(10)\n",
    "threshold = read_count * 0.001  # 0.1% threshold\n",
    "\n",
    "overrepresented = [(seq, count) for seq, count in most_common if count > threshold]\n",
    "\n",
    "# Status\n",
    "if len(overrepresented) == 0:\n",
    "    status = \"PASS\"\n",
    "    color = \"green\"\n",
    "elif len(overrepresented) <= 3:\n",
    "    status = \"WARN\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    status = \"FAIL\"\n",
    "    color = \"red\"\n",
    "\n",
    "# Display table\n",
    "print(f\"Status: {status}\\n\")\n",
    "print(f\"Overrepresented sequences (>{threshold:.0f} occurrences):\")\n",
    "print(f\"{'Sequence':<50} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "if overrepresented:\n",
    "    for seq, count in overrepresented:\n",
    "        pct = count / read_count * 100\n",
    "        seq_display = seq[:47] + '...' if len(seq) > 50 else seq\n",
    "        print(f\"{seq_display:<50} {count:>10,} {pct:>11.2f}%\")\n",
    "else:\n",
    "    print(\"No overrepresented sequences found.\")\n",
    "\n",
    "# Plot top 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "seqs = [f\"Seq {i+1}\" for i in range(len(most_common))]\n",
    "counts = [count for seq, count in most_common]\n",
    "plt.bar(seqs, counts, edgecolor='black', alpha=0.7)\n",
    "plt.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.0f})')\n",
    "plt.xlabel('Sequence Rank')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Top 10 Most Common Sequences - {status}', color=color, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "qc7-plot",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Control Summary Report\n",
    "\n",
    "**Understanding Quality Status:**\n",
    "\n",
    "- **PASS** (Green): Meets quality thresholds - data is good to use\n",
    "- **WARN** (Yellow): Slightly below optimal - review recommended but often acceptable\n",
    "- **FAIL** (Red): Significant quality issues - data may need filtering or be unusable\n",
    "\n",
    "**When to be concerned:**\n",
    "- 1-2 WARN metrics: Normal for many datasets, review those specific metrics\n",
    "- 3+ WARN metrics: Consider quality filtering before analysis\n",
    "- Any FAIL metrics: Investigate the issue, may need different dataset or quality trimming\n",
    "\n",
    "**Common WARN causes:**\n",
    "- Per-base quality: Normal quality degradation at read ends\n",
    "- GC content: Natural variation by organism\n",
    "- Duplication: Expected in RNA-seq (highly expressed genes)"
   ],
   "id": "summary-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate summary report with actual status tracking\n",
    "print(\"=\" * 70)\n",
    "print(\"QUALITY CONTROL SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: {accession}\")\n",
    "print(f\"Total reads analyzed: {read_count:,}\")\n",
    "print(f\"Average read length: {np.mean(sequence_lengths):.1f} bp\")\n",
    "print(f\"Average quality score: {np.mean([np.mean(q) for q in quality_scores]):.1f}\")\n",
    "print(f\"Average GC content: {np.mean(gc_contents):.1f}%\")\n",
    "print(f\"\\nQC Metrics Summary:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<45} {'Status':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calculate statuses (these match the logic in each metric above)\n",
    "median_quality = np.median([np.median(q) for q in quality_scores])\n",
    "qc1_status = \"PASS\" if median_quality >= 28 else \"WARN\" if median_quality >= 20 else \"FAIL\"\n",
    "\n",
    "peak_quality = np.percentile([np.mean(q) for q in quality_scores], 50)\n",
    "qc2_status = \"PASS\" if peak_quality >= 30 else \"WARN\" if peak_quality >= 20 else \"FAIL\"\n",
    "\n",
    "# Per-base content variation\n",
    "positions = sorted(per_base_content.keys())\n",
    "a_pct = [per_base_content[pos]['A'] / sum(per_base_content[pos].values()) * 100 for pos in positions]\n",
    "max_variation = np.std(a_pct)\n",
    "qc3_status = \"PASS\" if max_variation < 10 else \"WARN\" if max_variation < 20 else \"FAIL\"\n",
    "\n",
    "std_gc = np.std(gc_contents)\n",
    "qc4_status = \"PASS\" if std_gc < 10 else \"WARN\" if std_gc < 15 else \"FAIL\"\n",
    "\n",
    "unique_lengths = len(Counter(sequence_lengths))\n",
    "qc5_status = \"PASS\" if unique_lengths == 1 else \"WARN\" if unique_lengths <= 3 else \"FAIL\"\n",
    "\n",
    "total_sequences = len(sequence_counts)\n",
    "unique_sequences = sum(1 for count in sequence_counts.values() if count == 1)\n",
    "duplication_rate = (1 - unique_sequences / total_sequences) * 100\n",
    "qc6_status = \"PASS\" if duplication_rate < 20 else \"WARN\" if duplication_rate < 50 else \"FAIL\"\n",
    "\n",
    "threshold = read_count * 0.001\n",
    "overrepresented = [(seq, count) for seq, count in sequence_counts.most_common(10) if count > threshold]\n",
    "qc7_status = \"PASS\" if len(overrepresented) == 0 else \"WARN\" if len(overrepresented) <= 3 else \"FAIL\"\n",
    "\n",
    "# Print all statuses\n",
    "metrics_status = [\n",
    "    (\"1. Per-base Quality Scores\", qc1_status),\n",
    "    (\"2. Per-sequence Quality Distribution\", qc2_status),\n",
    "    (\"3. Per-base Sequence Content\", qc3_status),\n",
    "    (\"4. GC Content Distribution\", qc4_status),\n",
    "    (\"5. Sequence Length Distribution\", qc5_status),\n",
    "    (\"6. Sequence Duplication Levels\", qc6_status),\n",
    "    (\"7. Overrepresented Sequences\", qc7_status)\n",
    "]\n",
    "\n",
    "for metric, status in metrics_status:\n",
    "    print(f\"{metric:<45} {status:>10}\")\n",
    "\n",
    "# Overall assessment\n",
    "pass_count = sum(1 for _, s in metrics_status if s == \"PASS\")\n",
    "warn_count = sum(1 for _, s in metrics_status if s == \"WARN\")\n",
    "fail_count = sum(1 for _, s in metrics_status if s == \"FAIL\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Overall: {pass_count} PASS, {warn_count} WARN, {fail_count} FAIL\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nRecommendations:\")\n",
    "if fail_count > 0:\n",
    "    print(\"  \u26a0 FAIL metrics detected - review data quality before analysis\")\n",
    "if warn_count > 0:\n",
    "    print(\"  \u26a0 WARN metrics detected - consider quality filtering\")\n",
    "if pass_count == 7:\n",
    "    print(\"  \u2713 All metrics passed - data quality is excellent\")\n",
    "print(\"  \u2022 Review individual metrics above for detailed information\")\n",
    "print(\"  \u2022 Consider adapter trimming if overrepresented sequences detected\")\n",
    "print(\"  \u2022 Low quality bases may need trimming before downstream analysis\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "summary",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cleanup Environment\n",
    "\n",
    "To avoid incurring costs associated with the environment created in your account, delete the CloudFormation Stack which will remove all created resources. Execute the cells below to clean up local files and S3 buckets.\n",
    "\n",
    "**Important**: If the S3 buckets created by CloudFormation are not empty, the Delete step for CloudFormation will fail."
   ],
   "id": "part5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Local Files"
   ],
   "id": "cleanup-local-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean up local files\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Cleaning up local files...\")\n",
    "\n",
    "# Go back to parent directory\n",
    "os.chdir('..')\n",
    "\n",
    "# Remove sra_data directory\n",
    "if os.path.exists('sra_data'):\n",
    "    shutil.rmtree('sra_data')\n",
    "    print(\"\u2713 Removed sra_data directory\")\n",
    "else:\n",
    "    print(\"sra_data directory not found\")\n",
    "\n",
    "print(\"\\nLocal cleanup complete!\")"
   ],
   "id": "cleanup-local",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean S3 Buckets"
   ],
   "id": "cleanup-s3-header"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Comprehensive cleanup for versioned S3 buckets\n",
    "def cleanup_versioned_bucket(bucket_name):\n",
    "    \"\"\"Delete all objects and versions from a versioned S3 bucket\"\"\"\n",
    "    print(f\"Starting cleanup of bucket: {bucket_name}\")\n",
    "    \n",
    "    try:\n",
    "        # First, list all current objects\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            print(f\"Found {len(response['Contents'])} current objects\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"  - {obj['Key']}\")\n",
    "        else:\n",
    "            print(\"No current objects found\")\n",
    "        \n",
    "        # List and delete all object versions\n",
    "        paginator = s3.get_paginator('list_object_versions')\n",
    "        pages = paginator.paginate(Bucket=bucket_name)\n",
    "        \n",
    "        delete_keys = []\n",
    "        total_versions = 0\n",
    "        total_markers = 0\n",
    "        \n",
    "        for page in pages:\n",
    "            # Delete object versions\n",
    "            if 'Versions' in page:\n",
    "                for version in page['Versions']:\n",
    "                    delete_keys.append({\n",
    "                        'Key': version['Key'], \n",
    "                        'VersionId': version['VersionId']\n",
    "                    })\n",
    "                    total_versions += 1\n",
    "            \n",
    "            # Delete delete markers\n",
    "            if 'DeleteMarkers' in page:\n",
    "                for marker in page['DeleteMarkers']:\n",
    "                    delete_keys.append({\n",
    "                        'Key': marker['Key'], \n",
    "                        'VersionId': marker['VersionId']\n",
    "                    })\n",
    "                    total_markers += 1\n",
    "        \n",
    "        print(f\"Found {total_versions} object versions and {total_markers} delete markers\")\n",
    "        \n",
    "        if delete_keys:\n",
    "            # Delete in batches of 1000 (AWS limit)\n",
    "            deleted_count = 0\n",
    "            for i in range(0, len(delete_keys), 1000):\n",
    "                batch = delete_keys[i:i+1000]\n",
    "                if batch:\n",
    "                    response = s3.delete_objects(\n",
    "                        Bucket=bucket_name,\n",
    "                        Delete={'Objects': batch}\n",
    "                    )\n",
    "                    deleted_count += len(batch)\n",
    "                    print(f\"Deleted batch of {len(batch)} objects/versions\")\n",
    "            \n",
    "            print(f\"Successfully deleted {deleted_count} total objects/versions from {bucket_name}\")\n",
    "        else:\n",
    "            print(f\"No objects or versions to delete in {bucket_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up {bucket_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Clean up both buckets\n",
    "print(\"=\" * 70)\n",
    "print(\"S3 BUCKET CLEANUP\")\n",
    "print(\"=\" * 70)\n",
    "cleanup_versioned_bucket(dest_bucket)\n",
    "print()\n",
    "\n",
    "# Construct logging bucket name\n",
    "account_id = dest_bucket.split('-')[0]\n",
    "logging_bucket = f\"{account_id}-opendata-sra-logs\"\n",
    "cleanup_versioned_bucket(logging_bucket)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"CLEANUP COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nYou can now safely delete the CloudFormation stack.\")\n",
    "print(\"\\nTo delete the stack, run:\")\n",
    "print(\"aws cloudformation delete-stack --stack-name opendata-sra-notebook-stack\")"
   ],
   "id": "cleanup-s3",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}